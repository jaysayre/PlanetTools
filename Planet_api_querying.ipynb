{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### planet_api_querying.py --- Last updated 2/27/21\n",
    "Contact: James Sayre, sayrejay@gmail.com\n",
    "\n",
    "#### Written to most efficiently create a patchwork of satellite images over our target area.\n",
    "\n",
    "#### Inputs: \n",
    "\"data/muncodes/shp/MUNICIPIOS.shp\" -- INEGI provided shapefile of municipality shapes, municipality codes + names\n",
    "\n",
    "\"Intermediates/search_regions_img_corr.csv\" -- produced by create_suitability_index.py, a dataset listing all of the target boxes (each box is 0.008333333 degrees by 0.008333333 degrees, or <a href=\"https://en.wikipedia.org/wiki/Decimal_degrees\">approx</a> 1km sq.) we are interested in (including constraints on agricultural suitability, excluding urban areas, water, etc.) for all of Mexico. Each box is associated with the municipality it falls most within, by the variable \"muncode\" (i.e. the municipality code). x and y in the variable names represent the lat and lon coordinates, i and j represent the ordering of the boxes in the x and y dimensions respectively, and bid is the unique \n",
    "\n",
    "#### Outputs:\n",
    "For each municipality queried, in \"Remote Sensing/Planet/\":\n",
    "\n",
    "1) Creates a folder with the state code (i.e. if the municipality code is 20519), creates the folder \"20\"\n",
    "\n",
    "2) Within that folder, creates a subfolder with the municipality code, and within that, an \"Input\" folder\n",
    "\n",
    "3) Creates folders for each growing season and year\n",
    "\n",
    "4) Within each subfolder, creates a csv called mun_boxes_CYCLE_NAME_YEAR.csv which details, for the relevant boxes in \"search_regions_img_corr.csv\" (i.e. the ones with the municipality code 20519) the correspondence between each box (the unique code for each box in this file is bid) and the satellite image it is contained in for that crop cycle and year (found within image_id). Note that some boxes may have multiple satellite images they are contained in.\n",
    "\n",
    "5) Within the same subfolder, creates a shapefile called \"search_results_CYCLE_NAME_YEAR.shp\", which details each of the relevant satellite images for a given municipality and crop cycle. The outline of each of the satellite images is plotted in the image file \"search_results_CYCLE_NAME_YEAR.png\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests as re\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import time\n",
    "# import regionmask\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import cascaded_union\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from packaging import version\n",
    "# from functools import partial\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "### Directories\n",
    "base_dir = \"~/\"\n",
    "remote_sen_input_dir = os.path.join(base_dir, \"Remote Sensing\", \"Input/\")\n",
    "remote_sen_output_dir = os.path.join(base_dir, \"Remote Sensing\", \"Output/\")\n",
    "image_dir = os.path.join(base_dir, \"Remote Sensing\", \"Images/\")\n",
    "\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "intermediate_dir = os.path.join(base_dir, \"Intermediates\")\n",
    "\n",
    "### Inputs\n",
    "mun_shp =  os.path.join(data_dir, \"muncodes\",\"shp\",\"MUNICIPIOS.shp\")\n",
    "search_region_satelite_imgs = os.path.join(intermediate_dir,\"search_regions_img_corr.csv\")\n",
    "\n",
    "### Params\n",
    "eng_or_esp = \"eng\" ###eng for english, else for spanish \n",
    "### Fetch planet key stored in local environment so it's not shared with other users\n",
    "PLANET_API_KEY = os.environ['PL_API_KEY_James']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Programs\n",
    "def return_date_id(image_id):\n",
    "    parts = image_id.split('_')[0]\n",
    "    year,month, day = parts[:4], parts[4:6], parts[6:]\n",
    "    return int(month), int(day), int(year)\n",
    "\n",
    "def gt(dt_str):\n",
    "    dt, _, us = dt_str.partition(\".\")\n",
    "    dt = datetime.datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    us = int(us.rstrip(\"Z\"), 10)\n",
    "    return dt + datetime.timedelta(microseconds=us)\n",
    "        \n",
    "def return_overlap_prcnt(sat_polygon,mun_shp):\n",
    "    ### Computes the percentage overlap between a gpd dataframe (mun_shp, with one obs) and shapely polygon (sat_polygon)\n",
    "    df_poly = mun_shp.loc[mun_shp.index[0],'geometry']\n",
    "    return sat_polygon.intersection(df_poly).area/df_poly.area\n",
    "\n",
    "\n",
    "def search_planet_images(gteyr,gtemnth,gteday,lteyr,ltemnth,lteday,\n",
    "                         cloud_coverage=0.65,\n",
    "                         subsetpoly=None,subsetdf=None, itemtype=\"PSScene4Band\",epsg=4326):\n",
    "    aoi = {'crs': {'type': 'name', 'properties': {'name': 'urn:ogc:def:crs:EPSG::'+str(epsg)}},\n",
    "     'type': 'Polygon'}\n",
    "\n",
    "    if subsetpoly != None:\n",
    "        aoi['coordinates'] = shapely.geometry.mapping(subsetpoly)['coordinates'] ## poly\n",
    "    else:\n",
    "        aoi['coordinates'] = shapely.geometry.mapping(subsetdf.loc[subsetdf.index[0],'geometry'])['coordinates'] ##df\n",
    "\n",
    "    geometry_filter = {\n",
    "      \"type\": \"GeometryFilter\",\n",
    "      \"field_name\": \"geometry\",\n",
    "      \"config\": aoi\n",
    "    }\n",
    "\n",
    "    # get images acquired within a date range\n",
    "    date_range_filter = {\n",
    "      \"type\": \"DateRangeFilter\",\n",
    "      \"field_name\": \"acquired\",\n",
    "      \"config\": {\n",
    "        \"gte\": datetime.datetime(year=gteyr, month=gtemnth, day=gteday).isoformat()+'.000Z',\n",
    "        \"lte\": datetime.datetime(year=lteyr, month=ltemnth, day=lteday).isoformat()+'.000Z'\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # only get images which have <50% cloud coverage\n",
    "    cloud_cover_filter = {\n",
    "      \"type\": \"RangeFilter\",\n",
    "      \"field_name\": \"cloud_cover\",\n",
    "      \"config\": {\n",
    "        \"lte\": cloud_coverage\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # combine our geo, date, cloud filters\n",
    "    combined_filter = {\n",
    "      \"type\": \"AndFilter\",\n",
    "      \"config\": [geometry_filter, date_range_filter, cloud_cover_filter]\n",
    "    }\n",
    "\n",
    "    item_type = itemtype\n",
    "\n",
    "    # API request object\n",
    "    search_request = {\n",
    "      \"item_types\": [item_type], \n",
    "      \"filter\": combined_filter\n",
    "    }\n",
    "\n",
    "    # fire off the POST request\n",
    "    try:\n",
    "        search_result = \\\n",
    "          re.post('https://api.planet.com/data/v1/quick-search',\n",
    "          auth=HTTPBasicAuth(PLANET_API_KEY, ''),\n",
    "          json=search_request)\n",
    "        results = [feature for feature in search_result.json()['features']]\n",
    "    except:\n",
    "        try:\n",
    "            search_result = \\\n",
    "              re.post('https://api.planet.com/data/v1/quick-search',\n",
    "              auth=HTTPBasicAuth(PLANET_API_KEY, ''),\n",
    "              json=search_request)\n",
    "            results = [feature for feature in search_result.json()['features']]\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(5)\n",
    "                search_result = \\\n",
    "                  re.post('https://api.planet.com/data/v1/quick-search',\n",
    "                  auth=HTTPBasicAuth(PLANET_API_KEY, ''),\n",
    "                  json=search_request)\n",
    "\n",
    "                results = [feature for feature in search_result.json()['features']]\n",
    "            except:\n",
    "                results = []\n",
    "    return results\n",
    "\n",
    "def build_surrounding_box_df(dataframe,delta=0.008333333, epsg=4326):\n",
    "    \"\"\"For given lat/lon coordinates of a target area, builds a shapely polygon of that box and adds as col\n",
    "    --Inputs: dataframe -- pandas dataframe containing x/lon and y/lat coordinates of target area boxes\n",
    "    --delta -- float specifying width/height of box (in lat/lon coords)\n",
    "    -- Outputs: geopandas dataframe, with geometry supplying the shapely box for each target area\"\"\"\n",
    "    ytop, ybottom = dataframe['y']+(delta/2.0), dataframe['y']-(delta/2.0)\n",
    "    xleft, xright = dataframe['x']-(delta/2.0), dataframe['x']+(delta/2.0)\n",
    "    dataframe['geometry'] =  Polygon([(xleft, ytop), (xright, ytop), (xright, ybottom), (xleft, ybottom)])\n",
    "    dataframe.crs = \"EPSG:\"+str(epsg)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def return_relevant_shapes(result_list, subsetpoly, subsetdf, already_downloaded_ids=[], return_epsg=0,\n",
    "                          override_filter=False,below_cc_no_dup_threshold=0.10,med_dup_threshold=0.2):\n",
    "    shapes = []\n",
    "    ccs = []\n",
    "    ses = []\n",
    "    prcnt_covered_muns = []\n",
    "    ids = []\n",
    "    epsgs = []\n",
    "    \n",
    "    for result in result_list:\n",
    "        p_filename = result['id']\n",
    "        if len(result['geometry']['coordinates'][0]) == 1:\n",
    "            shape = Polygon(result['geometry']['coordinates'][0][0])\n",
    "        else:\n",
    "            shape = Polygon(result['geometry']['coordinates'][0])\n",
    "        prcnt_covered_mun = return_overlap_prcnt(shape,mun_shp=subsetdf) ### df\n",
    "        prcnt_covered_poly = shape.intersection(subsetpoly).area/subsetpoly.area ### poly\n",
    "        cc = result['properties']['cloud_cover']\n",
    "        se= result['properties']['sun_elevation']\n",
    "        epsg = result['properties']['epsg_code']\n",
    "        qual = result['properties']['quality_category']\n",
    "        if override_filter:\n",
    "            shapes.append(shape)\n",
    "            ccs.append(cc)\n",
    "            prcnt_covered_muns.append(-prcnt_covered_mun)\n",
    "            ids.append(p_filename)\n",
    "            ses.append(-se)\n",
    "            epsgs.append(epsg)\n",
    "        else:\n",
    "            if prcnt_covered_poly == 1:\n",
    "                if qual == \"standard\":\n",
    "                    shapes.append(shape)\n",
    "                    ccs.append(cc)\n",
    "                    prcnt_covered_muns.append(-prcnt_covered_mun)\n",
    "                    ids.append(p_filename)\n",
    "                    ses.append(-se)\n",
    "                    epsgs.append(epsg)\n",
    "\n",
    "\n",
    "    gdf = gpd.GeoDataFrame({'id':ids,'cloud_cov':ccs,'prcnt_cove':prcnt_covered_muns,\n",
    "                            'geometry':shapes,'sun_elev':ses,'epsg':epsgs})\n",
    "    gdf['sun_elev_round'] = (gdf['sun_elev']/5.0).apply(np.round)\n",
    "    \n",
    "    if override_filter:\n",
    "        gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove'])\n",
    "    else:\n",
    "        if return_epsg != 0:\n",
    "            gdf = gdf[gdf['epsg'] == return_epsg]\n",
    "        if len(already_downloaded_ids) >= 1:\n",
    "            if len(gdf[gdf['id'].isin(already_downloaded_ids)]) >= 1:\n",
    "                gdf = gdf[gdf['id'].isin(already_downloaded_ids)]\n",
    "            else:\n",
    "                if len(gdf[gdf['cloud_cov']<= below_cc_no_dup_threshold]) >= 1:\n",
    "                    gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove']).reset_index(drop=True)[0:1]\n",
    "                elif len(gdf[gdf['cloud_cov']<= med_dup_threshold]) >= 1:\n",
    "                    gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove']).reset_index(drop=True)[0:2]\n",
    "                else:\n",
    "                    gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove']).reset_index(drop=True)[0:3]\n",
    "        else:\n",
    "            if len(gdf[gdf['cloud_cov']<= below_cc_no_dup_threshold]) >= 1:\n",
    "                gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove']).reset_index(drop=True)[0:1]\n",
    "            elif len(gdf[gdf['cloud_cov']<= med_dup_threshold]) >= 1:\n",
    "                gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove']).reset_index(drop=True)[0:2]\n",
    "            else:\n",
    "                gdf = gdf.sort_values(['cloud_cov','sun_elev_round','prcnt_cove']).reset_index(drop=True)[0:3]\n",
    "    \n",
    "    gdf['prcnt_cove'] = -1.0*gdf['prcnt_cove']\n",
    "    gdf['sun_elev'] = -1.0*gdf['sun_elev']\n",
    "    \n",
    "    gdf.drop('sun_elev_round',1,inplace=True)\n",
    "    gdf['muncode'] = subsetdf['muncode'][list(subsetdf.index)[0]]\n",
    "    gdf['mun_name'] = subsetdf['NOM_MUN'][list(subsetdf.index)[0]]\n",
    "    return gdf\n",
    "\n",
    "def check_smaller_cover(dataframe,box_poly,already_downloaded_ids=[]):\n",
    "    \"\"\"Checks if, for the satellite images detailed in dataframe,\n",
    "    there exists a smaller number of images that will still cover all of box_poly\"\"\"\n",
    "    if len(dataframe[dataframe['cloud_cov'] > 0.10]) >= 1:\n",
    "        above_cloud_threshold_df = dataframe[dataframe['cloud_cov'] > 0.10]\n",
    "    else:\n",
    "        above_cloud_threshold_df = pd.DataFrame()\n",
    "    above_cloud_threshold_df['indep_cvrg'] = 0.0\n",
    "    \n",
    "    dataframe = dataframe[dataframe['cloud_cov'] <= 0.10]\n",
    "    dataframe['indep_cvrg'] = 0.0\n",
    "    duplicate_is = []\n",
    "    for i in dataframe['geometry'].index:\n",
    "        intersects= []\n",
    "        for j in dataframe['geometry'].index:\n",
    "            if i != j:\n",
    "                if j not in duplicate_is:\n",
    "                    overlap_area = dataframe['geometry'][i].intersection(dataframe['geometry'][j]).area\n",
    "                    if overlap_area > 0.0:\n",
    "                        intersects.append(str(j))\n",
    "\n",
    "        for j_id, j in enumerate(intersects):\n",
    "            if j_id == 0:\n",
    "                merged_poly = dataframe['geometry'][int(j)]\n",
    "            else: \n",
    "                merged_poly = merged_poly.union(dataframe['geometry'][int(j)])\n",
    "        if len(intersects) != 0:\n",
    "            if merged_poly.contains(dataframe['geometry'][i].intersection(box_poly)):\n",
    "                if len(already_downloaded_ids) > 0:\n",
    "                    if not dataframe.loc[i,'id'] in already_downloaded_ids:\n",
    "                        duplicate_is.append(i) \n",
    "                else:\n",
    "                    duplicate_is.append(i)\n",
    "            else:\n",
    "                non_indp_cov = merged_poly.intersection(dataframe['geometry'][i].intersection(box_poly)).area\n",
    "                full_cov = dataframe['geometry'][i].intersection(box_poly).area\n",
    "                dataframe.loc[i,'indep_cvrg'] =  (full_cov-non_indp_cov)##/box_poly.area\n",
    "                \n",
    "    dataframe.drop(duplicate_is, 0, inplace=True)\n",
    "    dataframe = dataframe.append(above_cloud_threshold_df,sort=False).reset_index(drop=True)\n",
    "    return dataframe\n",
    "\n",
    "def plot_sat_img_coverage(mun_outline_df,target_boxes_df,satellite_df,plot_path,lang=eng_or_esp):\n",
    "    '''Make a plot of the municipality outline, the target boxes, and the satellite assets\n",
    "    Inputs:\n",
    "    mun_outline_df -- geopandas dataframe with only observation the municipality and associated shapefile\n",
    "    target_boxes_df -- geopandas dataframe with the target boxes and associated shapefile\n",
    "    satellite_df -- geopandas dataframe with associated information on each satellite asset and shapefile\n",
    "    plot_path -- string pointing to where to plot the image\n",
    "    lang -- string, looks for either \"eng\" or else plots in Spanish'''\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    mun_outline_df.plot(ax=ax,color='white', edgecolor='blue', alpha=0.7)\n",
    "    gpd.GeoDataFrame(target_boxes_df).plot(ax=ax,color='red', edgecolor='white', alpha=0.7)\n",
    "    satellite_df[satellite_df['cloud_cov'] <= 0.10].plot(ax=ax,color='green', edgecolor='white',alpha=0.4)\n",
    "    satellite_df[satellite_df['cloud_cov'] > 0.10].plot(ax=ax,color='blue', edgecolor='white',alpha=0.2)\n",
    "    \n",
    "    centroid_mun = mun_shape_df['geometry'][mun_shape_df.index[0]].centroid\n",
    "    xcenter, ycenter = centroid_mun.x, centroid_mun.y\n",
    "    \n",
    "    datesrt = str(start_yr)+\"-\"+str(start_mes)+\"-\"+str(start_day)\n",
    "    dateend = str(end_yr)+\"-\"+str(end_mes)+\"-\"+str(end_day)\n",
    "    if lang == \"eng\":\n",
    "        plt_text = 'Available satellite images for '+mun_name+', from '+datesrt+' to '+dateend+', '+str(np.round(ycenter,2))+'N '+str(np.round(xcenter,2))+'W '\n",
    "    else:\n",
    "        plt_text ='Imágenes satélites disponibles de '+mun_name+' municipio '+datesrt+' a '+dateend+', '+str(np.round(ycenter,2))+'N '+str(np.round(xcenter,2))+'W'\n",
    "\n",
    "    plt.title(plt_text, fontsize=10)    \n",
    "    plt.savefig(plot_path, dpi=600)\n",
    "    \n",
    "\n",
    "def query_all_boxes(all_satellite_df,mun_satellite_df,mun_box_df,mun_df,\n",
    "                    start_yr, start_mes, start_day, end_yr, end_mes, end_day,img_fls_dled,force_epsg,\n",
    "                    muncode,mun_name,below_cc_no_dup_threshold=0.10,med_dup_threshold=0.2):\n",
    "    \"\"\"For each box, checks whether 1) it is already covered by a satellite image from a neighboring\n",
    "    municipality or 2) a satellite image from the same municipality and if not, it queries the Planet\n",
    "    API for satellite imagery that could cover that box\n",
    "    -- Inputs: all_satellite_df -- geopandas dataframe with information on the images from other states\n",
    "    -mun_satellite_df -- geopandas dataframe with information on the images from just the mun of interest\n",
    "    -- mun_box_df -- geopandas dataframe with the target boxes of interest\n",
    "    --- Outputs: mun_satellite_df, with images on the mun of interest\"\"\"\n",
    "    print(\"Querying box:\")\n",
    "    for id_box in mun_box_df.index:\n",
    "        print(id_box),\n",
    "        subset_p = mun_box_df.loc[id_box,'geometry']\n",
    "        ### If box contained in any state wide images, append these first to the list of municipality satellite imgs\n",
    "        if len(all_satellite_df[all_satellite_df['geometry'].apply(lambda x: x.contains(subset_p))]) >=1:\n",
    "            shapes = all_satellite_df[all_satellite_df['geometry'].apply(lambda x: x.contains(subset_p))].reset_index(drop=True)\n",
    "            shapes['muncode'] = muncode\n",
    "            shapes['mun_name'] = mun_name\n",
    "            mun_satellite_df = mun_satellite_df.append(shapes,sort=False)\n",
    "            mun_satellite_df = mun_satellite_df.drop_duplicates(subset='id')\n",
    "        \n",
    "        box_in_sat_img_df = mun_satellite_df[mun_satellite_df['geometry'].apply(lambda x: x.contains(subset_p))]\n",
    "        ### If box contained in 3 or more images, nothing to do here     \n",
    "        if len(box_in_sat_img_df) >=3:\n",
    "             pass\n",
    "        ### If box contained in 2 images and 1 below high cloud threshold pass, else download another image(s)     \n",
    "        elif len(box_in_sat_img_df) == 2:\n",
    "            if len(box_in_sat_img_df[box_in_sat_img_df['cloud_cov'] <= med_dup_threshold]) >= 1:\n",
    "                pass\n",
    "            else:\n",
    "                results = search_planet_images(start_yr,start_mes,start_day,end_yr,end_mes,end_day,subsetpoly=subset_p)\n",
    "                shapes = return_relevant_shapes(results,subset_p,mun_df,img_fls_dled,force_epsg)\n",
    "                mun_satellite_df = mun_satellite_df.append(shapes,sort=False)\n",
    "                mun_satellite_df = mun_satellite_df.drop_duplicates(subset='id')\n",
    "                \n",
    "        ### If box contained in 1 images and it is below medium cloud threshold pass, else download another image(s) \n",
    "        elif len(box_in_sat_img_df) == 1:\n",
    "            if len(box_in_sat_img_df[box_in_sat_img_df['cloud_cov'] <= below_cc_no_dup_threshold]) >= 1:\n",
    "                pass\n",
    "            else:\n",
    "                results = search_planet_images(start_yr,start_mes,start_day,end_yr,end_mes,end_day,subsetpoly=subset_p)\n",
    "                shapes = return_relevant_shapes(results,subset_p,mun_df,img_fls_dled,force_epsg)\n",
    "                mun_satellite_df = mun_satellite_df.append(shapes,sort=False)\n",
    "                mun_satellite_df = mun_satellite_df.drop_duplicates(subset='id')\n",
    "        else:\n",
    "            results = search_planet_images(start_yr,start_mes,start_day,end_yr,end_mes,end_day,subsetpoly=subset_p)\n",
    "            shapes = return_relevant_shapes(results,subset_p,mun_df,img_fls_dled,force_epsg)\n",
    "            mun_satellite_df = mun_satellite_df.append(shapes,sort=False)\n",
    "            mun_satellite_df = mun_satellite_df.drop_duplicates(subset='id')\n",
    "    return mun_satellite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in shapefile for Mexico\n",
    "df = gpd.read_file(mun_shp)\n",
    "### We need to reproject this shapefile into lat/lon coordinates, or whatever other projection you want\n",
    "df = df.to_crs(epsg=4326)\n",
    "df['muncode'] = df['CVE_ENT']+df['CVE_MUN']\n",
    "df['muncode'] = df['muncode'].astype(int)\n",
    "\n",
    "df['area'] = df.geometry.area \n",
    "df.sort_values('muncode', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_reg_df = pd.read_csv(search_region_satelite_imgs) ### Grid cells for Mexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crop cycles for Mexico\n",
    "crop_cycles = [(2016, 7, 1, 2016, 7, 20, 'summer_start_16'),(2016, 10, 10, 2016, 11, 1, 'summer_end_16'),\n",
    "               (2016, 11, 15, 2016, 12, 5,'winter_start_17'),(2017, 2, 10, 2017, 3, 1, 'winter_end_17'),\n",
    "               (2017, 3, 11, 2017, 4, 1,'spring_start_17'),(2017, 5, 20, 2017, 6, 10, 'spring_end_17'),\n",
    "               (2017, 6, 20, 2017, 7, 10,'summer_start_17'),(2017, 10, 10, 2017, 11, 1, 'summer_end_17'),\n",
    "               (2017, 11, 15, 2017, 12, 5,'winter_start_18'),(2018, 2, 10, 2018, 3, 1, 'winter_end_18'),\n",
    "               (2018, 3, 11, 2018, 4, 1,'spring_start_18'),(2018, 5, 20, 2018, 6, 10, 'spring_end_18'),\n",
    "               (2018, 6, 20, 2018, 7, 10,'summer_start_18'),(2018, 10, 10, 2018, 11, 1, 'summer_end_18'),\n",
    "               (2018, 11, 15, 2018, 12, 5,'winter_start_19'),(2019, 2, 10, 2019, 3, 1, 'winter_end_19'),\n",
    "               (2019, 3, 11, 2019, 4, 1,'spring_start_19'),(2019, 5, 20, 2019, 6, 10, 'spring_end_19'),\n",
    "               (2019, 6, 20, 2019, 7, 10,'summer_start_19'),(2019, 10, 10, 2019, 11, 1, 'summer_end_19'),\n",
    "               (2019, 11, 15, 2019, 12, 5,'winter_start_20'),(2020,  2, 10, 2020, 3, 1, 'winter_end_20'),\n",
    "               (2020, 3, 11, 2020, 4, 1,'spring_start_20'),(2020, 5, 20, 2020, 6, 10, 'spring_end_20'),\n",
    "               (2020, 6, 20, 2020, 7, 10,'summer_start_20'),(2020, 10, 10, 2020, 11, 1, 'summer_end_20'),\n",
    "               (2020, 11, 15, 2020, 12, 5,'winter_start_21'),(2021,  2, 10, 2021, 3, 1, 'winter_end_21')]\n",
    "\n",
    "crop_cycles.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_full_query(list_of_muncodes,crop_cycles=crop_cycles,plot_cov_png = False,\n",
    "                   force_epsg = 0,image_dir=image_dir,\n",
    "                   remote_sens_input_dir=remote_sen_input_dir):\n",
    "    ### Use force_epsg = 0, if you don't want to force having only results for one epsg code\n",
    "    # e.g. force_epsg = 32647 \n",
    "    for start_yr, start_mes, start_day, end_yr, end_mes, end_day, ciclo_name in crop_cycles:    \n",
    "        query_shapes_df = gpd.GeoDataFrame({'geometry':[]})\n",
    "        for muncode in list_of_muncodes:\n",
    "            state_code = str(muncode).replace(str(muncode)[-3:],\"\")+\"/\"\n",
    "            fullmuncode = str(muncode)+\"/\"\n",
    "            name_col = 'NOM_MUN'\n",
    "            if os.path.exists(image_dir+state_code):\n",
    "                img_fls_dled = []\n",
    "                for r,d,f in os.walk(image_dir+state_code):\n",
    "                    for file in f:\n",
    "                        if '_sr.tif' in file:\n",
    "                            img_fls_dled.append(file)                \n",
    "                img_fls_dled = [img_fl.split('_sr.tif')[0] for img_fl in img_fls_dled]\n",
    "            else:\n",
    "                img_fls_dled = []\n",
    "            output_fl_dir = remote_sens_input_dir+state_code+fullmuncode+ciclo_name+\"/\"\n",
    "            if not os.path.exists(output_fl_dir):\n",
    "                os.makedirs(output_fl_dir)\n",
    "\n",
    "            if not os.path.isfile(output_fl_dir+'search_results_'+ciclo_name+'.shp'):\n",
    "                print(\"Now running muncode:\", muncode, \"and cycle:\", ciclo_name)\n",
    "                query_shapes_municipial_df= gpd.GeoDataFrame({'geometry':[]})\n",
    "\n",
    "                mun_search_df = search_reg_df[search_reg_df['muncode'] == muncode].reset_index(drop=True)\n",
    "                mun_search_df = mun_search_df.apply(build_surrounding_box_df,axis=1)  ### build shapely box for each target area\n",
    "                mun_shape_df = df[df['muncode'] == muncode]        \n",
    "                mun_name = df.loc[list(df[df['muncode'] == muncode].index)[0],name_col]\n",
    "\n",
    "                query_shapes_municipial_df = query_all_boxes(query_shapes_df,query_shapes_municipial_df,mun_search_df,mun_shape_df,\n",
    "                                                            start_yr, start_mes, start_day, end_yr, end_mes, end_day,\n",
    "                                                             img_fls_dled, force_epsg, muncode, mun_name)\n",
    "\n",
    "                query_shapes_municipial_df = query_shapes_municipial_df.reset_index(drop=True)\n",
    "                print(\"Checking smaller cover at:\",datetime.datetime.now())\n",
    "                try:\n",
    "                    query_shapes_municipial_df = check_smaller_cover(query_shapes_municipial_df,cascaded_union(list(mun_search_df['geometry'])),img_fls_dled)\n",
    "                    print(\"Finishing cover for\",muncode,\"at:\",datetime.datetime.now())\n",
    "\n",
    "                    ### Associate each box with the images that cover it\n",
    "                    for id_box in list(mun_search_df.index):\n",
    "                        subset_p = mun_search_df.loc[id_box,'geometry']\n",
    "                        shapes = query_shapes_municipial_df[query_shapes_municipial_df['geometry'].apply(lambda x: x.intersection(subset_p).area > 0.0)]\n",
    "                        if len(shapes) >=1:\n",
    "                            mun_search_df.loc[id_box,'image_id'] = ', '.join(list(shapes['id']))\n",
    "                except:\n",
    "                    print(\"HAD ISSUE WITH SMALLER COVER\", len(mun_search_df))\n",
    "\n",
    "                query_shapes_df = query_shapes_df.append(query_shapes_municipial_df,sort=False)\n",
    "                query_shapes_df = query_shapes_df.reset_index(drop=True)\n",
    "                if len(query_shapes_municipial_df) >= 1:\n",
    "                    print(query_shapes_municipial_df.columns)\n",
    "                    query_shapes_municipial_df['ciclo'] = ciclo_name\n",
    "                    query_shapes_municipial_df.to_file(output_fl_dir+'search_results_'+ciclo_name+'.shp',index=False)\n",
    "                    query_shapes_municipial_df.drop('geometry',1).to_csv(output_fl_dir+'search_results_'+ciclo_name+'.csv',index=False)\n",
    "                mun_search_df['ciclo'] = ciclo_name\n",
    "                mun_search_df.to_csv(output_fl_dir+'mun_boxes_'+ciclo_name+'.csv',index=False)\n",
    "\n",
    "                if plot_cov_png:\n",
    "                    plot_sat_img_coverage(mun_shape_df,mun_search_df,query_shapes_municipial_df,output_fl_dir+'search_results_'+ciclo_name+'.png')\n",
    "            else:\n",
    "                print(\"Skipping\", muncode, \"for cycle\", ciclo_name)\n",
    "                query_shapes_municipial_df = gpd.read_file(output_fl_dir+'search_results_'+ciclo_name+'.shp')\n",
    "                query_shapes_df = query_shapes_df.append(query_shapes_municipial_df,sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_muns = [2001, 2002, 2003, 2004, 2005] ### Baja California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgr = multiprocessing.Manager()\n",
    "pool_size = multiprocessing.cpu_count()\n",
    "split_ids = np.array_split(target_muns, pool_size)\n",
    "pool = multiprocessing.Pool(processes=pool_size)\n",
    "pool.map(run_full_query, split_ids)\n",
    "pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
